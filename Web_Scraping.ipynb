{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP90K93sCE4zRwxayT6K12O",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zoroaster-BGAE/Web-Scrapping-for-CFC/blob/main/Web_Scraping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WvF9EpU9UKdi"
      },
      "outputs": [],
      "source": [
        "#Clean One\n",
        "\n",
        "import requests\n",
        "import json\n",
        "from collections import defaultdict\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Scrape index webpage\n",
        "url = 'https://www.cfcunderwriting.com'\n",
        "response = requests.get(url)\n",
        "\n",
        "# Find all externally loaded resources\n",
        "soup = BeautifulSoup(response.text, 'lxml')\n",
        "externals = set()\n",
        "for tag in soup.find_all():\n",
        "    src = tag.get('src')\n",
        "    href = tag.get('href')\n",
        "    if src and src.startswith('http') and not src.startswith(url):\n",
        "        externals.add(src)\n",
        "    if href and href.startswith('http') and not href.startswith(url):\n",
        "        externals.add(href)\n",
        "\n",
        "# Write list of externals to a JSON file\n",
        "with open('externals.json', 'w') as f1:\n",
        "    json.dump(list(externals), f1)\n",
        "\n",
        "# Find the URL of the privacy policy page\n",
        "privacy_policy_url = None\n",
        "for a in soup.find_all('a'):\n",
        "    if a.text.lower() == 'privacy policy':\n",
        "        privacy_policy_url = a['href']\n",
        "\n",
        "#Correction 1 that checks if there is a http scheme and adjusts the url accordingly\n",
        "if not privacy_policy_url.startswith(\"http\"):\n",
        "    privacy_policy_url = \"https://cfcunderwriting.com\" + privacy_policy_url\n",
        "\n",
        "# Scrape the privacy policy page\n",
        "response_pp = requests.get(privacy_policy_url)\n",
        "soup_pp = BeautifulSoup(response_pp.text, 'html')\n",
        "\n",
        "# Create a case-insensitive word frequency count\n",
        "word_counts = defaultdict(int)\n",
        "for text in soup_pp.find_all(text=True):\n",
        "    for word in text.split():\n",
        "        word_counts[word.lower()] += 1\n",
        "\n",
        "# Write the word frequency count to a JSON file\n",
        "with open('word_counts.json', 'w') as f2:\n",
        "    json.dump(word_counts, f2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code first retrieves the index webpage and then uses BeautifulSoup to parse the HTML. It then looks for all tags that have a src or href attribute, and adds any external URLs to a set. It then writes this set to a JSON file.\n",
        "\n",
        "\n",
        "Next, the code searches the page for a hyperlink with the text \"Privacy Policy\" and gets the URL of this page. It then retrieves the privacy policy page and uses BeautifulSoup to parse the HTML. It then creates a case-insensitive word frequency count and writes this to a JSON file."
      ],
      "metadata": {
        "id": "o0S0qQDab5Uz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Ver 2\n",
        "\n",
        "'''\n",
        "\n",
        "import requests\n",
        "import re\n",
        "import json\n",
        "import collections\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Scrape the index webpage hosted at cfcunderwriting.com\n",
        "r = requests.get('http://cfcunderwriting.com')\n",
        "\n",
        "# Parse the HTML content\n",
        "soup = BeautifulSoup(r.text, 'html.parser')\n",
        "\n",
        "# Find all external resources (images, scripts, and fonts)\n",
        "external_resources = []\n",
        "for tag in soup.find_all():\n",
        "    if tag.name in ['img', 'script', 'link']:\n",
        "        src = tag.get('src')\n",
        "        href = tag.get('href')\n",
        "        if src and not src.startswith('http://cfcunderwriting.com'):\n",
        "            external_resources.append(src)\n",
        "        if href and not href.startswith('http://cfcunderwriting.com'):\n",
        "            external_resources.append(href)\n",
        "\n",
        "\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Write the list of external resources to a JSON file\n",
        "with open('external_resources.json', 'w') as f:\n",
        "    json.dump(external_resources, f)\n",
        "\n",
        "# Find the \"Privacy Policy\" page\n",
        "privacy_policy_url = None\n",
        "for a in soup.find_all('a'):\n",
        "    if a.text.lower() == 'privacy policy':\n",
        "        privacy_policy_url = a['href']\n",
        "\n",
        "\n",
        "#Correction 1: It checks if there is a http scheme\n",
        "if not privacy_policy_url.startswith(\"http\"):\n",
        "    privacy_policy_url = \"https://cfcunderwriting.com\" + privacy_policy_url\n",
        "\n",
        "# Scrape the \"Privacy Policy\" page\n",
        "if privacy_policy_url:\n",
        "    r = requests.get(privacy_policy_url)\n",
        "    soup = BeautifulSoup(r.text, 'html.parser')\n",
        "\n",
        "    # Extract the visible text from the page\n",
        "    text = soup.get_text()\n",
        "\n",
        "    # Remove non-alphabetic characters and split the text into words\n",
        "    words = re.sub(r'[^a-zA-Z\\s]', '', text).split()\n",
        "\n",
        "    # Create a word frequency count\n",
        "    word_count = collections.Counter(words)\n",
        "\n",
        "    # Write the word frequency count to a JSON file\n",
        "    with open('word_count.json', 'w') as f:\n",
        "        json.dump(word_count, f)"
      ],
      "metadata": {
        "id": "KslLNUxtcSIl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This program uses the requests library to make HTTP requests to the website, the re library to remove non-alphabetic characters from the text, and the collections library to create a word frequency count. It also uses the BeautifulSoup library to parse the HTML content of the web pages."
      ],
      "metadata": {
        "id": "CeSDG0oXcWoK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "soup = BeautifulSoup(response.text, 'lxml')\n",
        "externals = set()\n",
        "for tag in soup.find_all():\n",
        "    src = tag.get('src')\n",
        "    href = tag.get('href')\n",
        "    if src and src.startswith('http') and not src.startswith(url):\n",
        "        externals.add(src)\n",
        "    if href and href.startswith('http') and not href.startswith(url):\n",
        "        externals.add(href)\n",
        "\n",
        "print(len(externals), externals)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37ZDNmDV_zzO",
        "outputId": "466b6247-3ed2-44d2-f341-5726f93638fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15 {'https://fonts.googleapis.com/css?family=Montserrat:300,400,500,600,700', 'https://www.googletagmanager.com/ns.html?id=GTM-NGGN5FB', 'https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js', 'https://twitter.com/cfcunderwriting', 'https://ico.org.uk/your-data-matters/raising-concerns/', 'https://www.linkedin.com/company/cfc-underwriting-ltd/', 'https://www.cfcunderwriting.eu/', 'https://www.youtube.com/user/CFCUnderwriting', 'https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.1.3/css/bootstrap.min.css', 'https://www.google.com/recaptcha/api.js?render=6LemiyEaAAAAAGwb4nR8oX38fxyM36xjIGbwz6d4', 'https://support.google.com/analytics/answer/4597324?hl=en', 'https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.1.3/js/bootstrap.min.js', 'https://www.instagram.com/cfc_underwriting/', 'https://www.google.com/recaptcha/about/', 'https://www.facebook.com/cfcspecialistinsurance'}\n"
          ]
        }
      ]
    }
  ]
}