{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOa2f42OP8ZiMPmm3hZA9Ss",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zoroaster-BGAE/Web-Scrapping-for-CFC/blob/main/Web_Scraping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WvF9EpU9UKdi"
      },
      "outputs": [],
      "source": [
        "#Clean One\n",
        "\n",
        "import requests\n",
        "import json\n",
        "from collections import defaultdict\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Scrape index webpage\n",
        "url = 'https://www.cfcunderwriting.com'\n",
        "response = requests.get(url)\n",
        "\n",
        "# Find all externally loaded resources\n",
        "soup = BeautifulSoup(response.text, 'lxml')\n",
        "externals = set()\n",
        "for tag in soup.find_all():\n",
        "    src = tag.get('src')\n",
        "    href = tag.get('href')\n",
        "    if src and src.startswith('http') and not src.startswith(url):\n",
        "        externals.add(src)\n",
        "    if href and href.startswith('http') and not href.startswith(url):\n",
        "        externals.add(href)\n",
        "\n",
        "# Write list of externals to a JSON file\n",
        "with open('externals.json', 'w') as f1:\n",
        "    json.dump(list(externals), f1)\n",
        "\n",
        "# Find the URL of the privacy policy page\n",
        "privacy_policy_url = None\n",
        "for a in soup.find_all('a'):\n",
        "    if a.text.lower() == 'privacy policy':\n",
        "        privacy_policy_url = a['href']\n",
        "\n",
        "#Correction 1 that checks if there is a http scheme and adjusts the url accordingly\n",
        "if not privacy_policy_url.startswith(\"http\"):\n",
        "    privacy_policy_url = \"https://cfcunderwriting.com\" + privacy_policy_url\n",
        "\n",
        "# Scrape the privacy policy page\n",
        "response_pp = requests.get(privacy_policy_url)\n",
        "soup_pp = BeautifulSoup(response_pp.text, 'html')\n",
        "\n",
        "# Create a case-insensitive word frequency count\n",
        "word_counts = defaultdict(int)\n",
        "for text in soup_pp.find_all(text=True):\n",
        "    for word in text.split():\n",
        "        word_counts[word.lower()] += 1\n",
        "\n",
        "# Write the word frequency count to a JSON file\n",
        "with open('word_counts.json', 'w') as f2:\n",
        "    json.dump(word_counts, f2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code first retrieves the index webpage and then uses BeautifulSoup to parse the HTML. It then looks for all tags that have a src or href attribute, and adds any external URLs to a set. It then writes this set to a JSON file.\n",
        "\n",
        "\n",
        "Next, the code searches the page for a hyperlink with the text \"Privacy Policy\" and gets the URL of this page. It then retrieves the privacy policy page and uses BeautifulSoup to parse the HTML. It then creates a case-insensitive word frequency count and writes this to a JSON file."
      ],
      "metadata": {
        "id": "o0S0qQDab5Uz"
      }
    }
  ]
}